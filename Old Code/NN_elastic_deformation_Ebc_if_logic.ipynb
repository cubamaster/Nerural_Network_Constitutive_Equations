{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d92b52ca-2ca6-4a0f-9cee-38703eef52b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0 | total=1.2468e-01 | pde=1.7424e-01, A=0.0000e+00, D=3.2040e-03, B=4.3970e-20, C=2.7293e-20, corner=0.0000e+00\n",
      "Epoch   500 | total=1.3035e-05 | pde=1.8240e-05, A=0.0000e+00, D=9.5092e-08, B=2.4853e-20, C=2.5152e-20, corner=0.0000e+00\n",
      "Epoch  1000 | total=2.9595e-06 | pde=4.1315e-06, A=0.0000e+00, D=1.2966e-07, B=2.2694e-20, C=2.3223e-20, corner=0.0000e+00\n",
      "Epoch  1500 | total=1.3946e-06 | pde=1.9415e-06, A=0.0000e+00, D=1.2018e-07, B=2.2267e-20, C=2.2864e-20, corner=0.0000e+00\n",
      "Epoch  2000 | total=7.9037e-07 | pde=1.0969e-06, A=0.0000e+00, D=1.0579e-07, B=2.2245e-20, C=2.2932e-20, corner=0.0000e+00\n",
      "Epoch  2500 | total=4.9407e-07 | pde=6.8323e-07, A=0.0000e+00, D=9.3044e-08, B=2.2459e-20, C=2.3396e-20, corner=0.0000e+00\n",
      "Epoch  3000 | total=3.3084e-07 | pde=4.5658e-07, A=0.0000e+00, D=7.2502e-08, B=2.2511e-20, C=2.3396e-20, corner=0.0000e+00\n",
      "Epoch  3500 | total=2.1609e-07 | pde=2.9701e-07, A=0.0000e+00, D=6.0555e-08, B=2.2211e-20, C=2.3166e-20, corner=0.0000e+00\n",
      "Epoch  4000 | total=2.9926e-04 | pde=4.1896e-04, A=0.0000e+00, D=1.1111e-07, B=2.3312e-20, C=2.4885e-20, corner=0.0000e+00\n",
      "Epoch  4500 | total=8.4146e-08 | pde=1.1467e-07, A=0.0000e+00, D=3.4450e-08, B=2.3989e-20, C=2.4556e-20, corner=0.0000e+00\n",
      "Epoch  5000 | total=5.7674e-08 | pde=7.8008e-08, A=0.0000e+00, D=3.0058e-08, B=2.2690e-20, C=2.3382e-20, corner=0.0000e+00\n",
      "Epoch  5500 | total=1.1246e-07 | pde=1.5519e-07, A=0.0000e+00, D=2.4878e-08, B=2.0194e-20, C=2.1280e-20, corner=0.0000e+00\n",
      "Epoch  6000 | total=7.0476e-08 | pde=9.7166e-08, A=0.0000e+00, D=1.6486e-08, B=1.9524e-20, C=2.0593e-20, corner=0.0000e+00\n",
      "Epoch  6500 | total=3.6924e-08 | pde=5.0763e-08, A=0.0000e+00, D=1.0225e-08, B=1.9322e-20, C=2.0255e-20, corner=0.0000e+00\n",
      "Epoch  7000 | total=4.1883e-06 | pde=5.8631e-06, A=0.0000e+00, D=6.0120e-09, B=2.0234e-20, C=2.1623e-20, corner=0.0000e+00\n",
      "Epoch  7500 | total=1.7941e-08 | pde=2.4857e-08, A=0.0000e+00, D=2.8592e-09, B=1.9347e-20, C=2.0100e-20, corner=0.0000e+00\n",
      "Epoch  8000 | total=8.1477e-09 | pde=1.1276e-08, A=0.0000e+00, D=1.4321e-09, B=1.6837e-20, C=1.7643e-20, corner=0.0000e+00\n",
      "Epoch  8500 | total=4.8565e-08 | pde=6.7958e-08, A=0.0000e+00, D=3.5502e-10, B=1.6641e-20, C=1.7210e-20, corner=0.0000e+00\n",
      "Epoch  9000 | total=1.2007e-07 | pde=1.6808e-07, A=0.0000e+00, D=1.6293e-10, B=1.6712e-20, C=1.7298e-20, corner=0.0000e+00\n",
      "Epoch  9500 | total=7.5320e-09 | pde=1.0543e-08, A=0.0000e+00, D=1.7144e-11, B=1.8002e-20, C=1.8452e-20, corner=0.0000e+00\n",
      "         X     Y    u_x_actual    u_y_actual  u_x_pred  u_y_pred  \\\n",
      "0    -0.50 -0.25  1.483633e-19  1.263433e-19  0.000000 -0.000000   \n",
      "1    -0.48 -0.25  9.101041e-04  9.363998e-04  0.000504 -0.000241   \n",
      "2    -0.50 -0.23 -1.530000e-20 -4.190000e-20  0.000000 -0.000000   \n",
      "3    -0.48 -0.23  5.332043e-04  6.506182e-04  0.000504 -0.000241   \n",
      "4    -0.50 -0.21  2.321675e-21 -6.630000e-21  0.000000 -0.000000   \n",
      "...    ...   ...           ...           ...       ...       ...   \n",
      "1321  0.48  0.23  2.446680e-02 -6.510000e-04  0.024536 -0.011299   \n",
      "1322  0.50  0.21  2.500000e-02  3.680167e-18  0.025037  0.000000   \n",
      "1323  0.48  0.25  2.408990e-02 -9.360000e-04  0.024535 -0.011296   \n",
      "1324  0.50  0.23  2.500000e-02  1.768693e-18  0.025037  0.000000   \n",
      "1325  0.50  0.25  2.500000e-02  2.080774e-17  0.025036  0.000000   \n",
      "\n",
      "         error_u_x     error_u_y  percent_error_u_x  percent_error_u_y  \n",
      "0     1.483633e-19  1.263433e-19           0.000000           0.000000  \n",
      "1     4.064584e-04  1.177689e-03          44.660649         125.767758  \n",
      "2     1.530000e-20  4.190000e-20           0.000000           0.000000  \n",
      "3     2.965518e-05  8.916432e-04           5.561691         137.045538  \n",
      "4     2.321675e-21  6.630000e-21           0.000000           0.000000  \n",
      "...            ...           ...                ...                ...  \n",
      "1321  6.901301e-05  1.064783e-02           0.282068        1635.610747  \n",
      "1322  3.745630e-05  3.680167e-18           0.149825           0.000000  \n",
      "1323  4.451286e-04  1.036019e-02           1.847781        1106.857940  \n",
      "1324  3.669262e-05  1.768693e-18           0.146770           0.000000  \n",
      "1325  3.591403e-05  2.080774e-17           0.143656           0.000000  \n",
      "\n",
      "[1326 rows x 10 columns]\n",
      "Average Percent Error for u_x: 4.08%\n",
      "Average Percent Error for u_y: 1170.05%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Domain & Material Parameters\n",
    "# -----------------------------\n",
    "L, W = 1.0, 0.5\n",
    "lambda_ = 5.0e9\n",
    "mu = 5.0e9\n",
    "h  = 1.0\n",
    "\n",
    "# -----------------------------\n",
    "# PINN Model (same shape function)\n",
    "# -----------------------------\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.Softplus(beta=10),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Softplus(beta=10),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Softplus(beta=10),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x, y: shape (N,1)\n",
    "        inputs = torch.cat([x, y], dim=1)  # shape (N,2)\n",
    "        raw = self.net(inputs)             # shape (N,2)\n",
    "\n",
    "        enforced_out = torch.zeros_like(raw)\n",
    "        enforced_out[:, 0] = (x.flatten() + 0.5) * raw[:, 0]  # u_x\n",
    "        enforced_out[:, 1] = (x.flatten() + 0.5) * raw[:, 1]  # u_y\n",
    "\n",
    "        # Example corner constraint at x=0.5\n",
    "        corner_mask = (x[:, 0] == 0.5)\n",
    "        enforced_out[corner_mask, 1] = 0.0\n",
    "\n",
    "        return enforced_out\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Strain & Stress\n",
    "# -----------------------------\n",
    "def strain_tensor(u_x, u_y, x, y):\n",
    "    u_x_x = torch.autograd.grad(u_x, x,\n",
    "                                grad_outputs=torch.ones_like(u_x),\n",
    "                                retain_graph=True,\n",
    "                                create_graph=True)[0]\n",
    "    u_y_y = torch.autograd.grad(u_y, y,\n",
    "                                grad_outputs=torch.ones_like(u_y),\n",
    "                                retain_graph=True,\n",
    "                                create_graph=True)[0]\n",
    "    u_x_y = torch.autograd.grad(u_x, y,\n",
    "                                grad_outputs=torch.ones_like(u_x),\n",
    "                                retain_graph=True,\n",
    "                                create_graph=True)[0]\n",
    "    u_y_x = torch.autograd.grad(u_y, x,\n",
    "                                grad_outputs=torch.ones_like(u_y),\n",
    "                                retain_graph=True,\n",
    "                                create_graph=True)[0]\n",
    "\n",
    "    E_xx = u_x_x\n",
    "    E_yy = u_y_y\n",
    "    E_xy = 0.5 * (u_x_y + u_y_x)\n",
    "    return E_xx, E_yy, E_xy\n",
    "\n",
    "def stress_tensor(E_xx, E_yy, E_xy):\n",
    "    scale_factor = 1e9\n",
    "    trace_E = E_xx + E_yy\n",
    "    sigma_xx = h * ((lambda_/scale_factor)*trace_E + 2*(mu/scale_factor)*E_xx)\n",
    "    sigma_yy = h * ((lambda_/scale_factor)*trace_E + 2*(mu/scale_factor)*E_yy)\n",
    "    sigma_xy = h * (2*(mu/scale_factor)*E_xy)\n",
    "    return sigma_xx, sigma_yy, sigma_xy\n",
    "\n",
    "# -----------------------------\n",
    "# 2) PDE Residual (Interior)\n",
    "# -----------------------------\n",
    "def pde_loss_vectorized(model, x, y):\n",
    "    \"\"\"\n",
    "    Return PDE residual^2 per point (shape (N,)).\n",
    "    PDE: sigma_xx_x + sigma_xy_y = 0, sigma_yy_y + sigma_xy_x = 0\n",
    "    \"\"\"\n",
    "    x_ = x.clone().detach().requires_grad_(True)\n",
    "    y_ = y.clone().detach().requires_grad_(True)\n",
    "\n",
    "    u = model(x_, y_)\n",
    "    u_x = u[:, 0:1]\n",
    "    u_y = u[:, 1:2]\n",
    "\n",
    "    E_xx, E_yy, E_xy = strain_tensor(u_x, u_y, x_, y_)\n",
    "    sigma_xx, sigma_yy, sigma_xy = stress_tensor(E_xx, E_yy, E_xy)\n",
    "\n",
    "    sigma_xx_x = torch.autograd.grad(sigma_xx, x_,\n",
    "                                     grad_outputs=torch.ones_like(sigma_xx),\n",
    "                                     retain_graph=True,\n",
    "                                     create_graph=True)[0]\n",
    "    sigma_xy_y = torch.autograd.grad(sigma_xy, y_,\n",
    "                                     grad_outputs=torch.ones_like(sigma_xy),\n",
    "                                     retain_graph=True,\n",
    "                                     create_graph=True)[0]\n",
    "    sigma_yy_y = torch.autograd.grad(sigma_yy, y_,\n",
    "                                     grad_outputs=torch.ones_like(sigma_yy),\n",
    "                                     retain_graph=True,\n",
    "                                     create_graph=True)[0]\n",
    "    sigma_xy_x = torch.autograd.grad(sigma_xy, x_,\n",
    "                                     grad_outputs=torch.ones_like(sigma_xy),\n",
    "                                     retain_graph=True,\n",
    "                                     create_graph=True)[0]\n",
    "\n",
    "    residual_x = sigma_xx_x + sigma_xy_y\n",
    "    residual_y = sigma_yy_y + sigma_xy_x\n",
    "\n",
    "    return (residual_x**2 + residual_y**2).flatten()\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Boundary Losses (A,D,B,C)\n",
    "# -----------------------------\n",
    "def boundary_A_loss(model, x, y):\n",
    "    \"\"\"\n",
    "    A: x=-L/2 => (u_x=0, u_y=0)\n",
    "    => sum of squares of [u_x,u_y]\n",
    "    \"\"\"\n",
    "    u = model(x, y)\n",
    "    return torch.sum(u**2, dim=1)\n",
    "\n",
    "def boundary_D_loss(model, x, y, L):\n",
    "    \"\"\"\n",
    "    D: x=+L/2 => (u_x=0.025L, u_y=0)\n",
    "    => (u_x - 0.025L)^2 + (u_y)^2\n",
    "    \"\"\"\n",
    "    u = model(x, y)\n",
    "    diff_x = u[:,0] - (0.025*L)\n",
    "    diff_y = u[:,1]\n",
    "    return diff_x**2 + diff_y**2\n",
    "\n",
    "def boundary_B_loss(model, x, y):\n",
    "    \"\"\"\n",
    "    B: y=+W/2 => traction-free => sigma_yy=0, sigma_xy=0\n",
    "    => (sigma_yy^2 + sigma_xy^2)\n",
    "    \"\"\"\n",
    "    x_ = x.clone().detach().requires_grad_(True)\n",
    "    y_ = y.clone().detach().requires_grad_(True)\n",
    "    u = model(x_, y_)\n",
    "    u_x, u_y = u[:,0:1], u[:,1:2]\n",
    "\n",
    "    E_xx_B, E_yy_B, E_xy_B = strain_tensor(u_x, u_y, x_, y_)\n",
    "    sigma_xx_B, sigma_yy_B, sigma_xy_B = stress_tensor(E_xx_B, E_yy_B, E_xy_B)\n",
    "    return (sigma_yy_B**2 + sigma_xy_B**2).flatten()\n",
    "\n",
    "def boundary_C_loss(model, x, y):\n",
    "    \"\"\"\n",
    "    C: y=-W/2 => traction-free => sigma_yy=0, sigma_xy=0\n",
    "    \"\"\"\n",
    "    x_ = x.clone().detach().requires_grad_(True)\n",
    "    y_ = y.clone().detach().requires_grad_(True)\n",
    "    u = model(x_, y_)\n",
    "    u_x, u_y = u[:,0:1], u[:,1:2]\n",
    "\n",
    "    E_xx_C, E_yy_C, E_xy_C = strain_tensor(u_x, u_y, x_, y_)\n",
    "    sigma_xx_C, sigma_yy_C, sigma_xy_C = stress_tensor(E_xx_C, E_yy_C, E_xy_C)\n",
    "    return (sigma_yy_C**2 + sigma_xy_C**2).flatten()\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Corner Constraint\n",
    "# -----------------------------\n",
    "def corner_loss(model, x_corner, y_corner, weight=1e9):\n",
    "    \"\"\"\n",
    "    Enforce (u_x=0,u_y=0) at corner (x=-L/2,y=-W/2) with large weight\n",
    "    \"\"\"\n",
    "    u_c = model(x_corner, y_corner)  # shape (1,2)\n",
    "    return weight * torch.sum(u_c**2, dim=1)  # shape (1,)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Combined vectorized loss that returns\n",
    "#    (total_loss, sub_losses_dict)\n",
    "#    so you can print each sub-loss\n",
    "# -----------------------------\n",
    "def combined_loss_vectorized(model, x_all, y_all, L, W, corner_weight=1e9, tol=1e-6):\n",
    "    \"\"\"\n",
    "    1) PDE for interior\n",
    "    2) Boundaries:\n",
    "       A => x=-L/2, D => x=+L/2\n",
    "       B => y=+W/2 (traction-free)\n",
    "       C => y=-W/2 (traction-free)\n",
    "    3) Corner => (x=-L/2,y=-W/2) with large weight\n",
    "    We build a sub-loss dict: {pde, A, D, B, C, corner}.\n",
    "    Then total_loss = mean(...) + corner_loss\n",
    "    \"\"\"\n",
    "    # sub-loss dictionary to store mean of each portion\n",
    "    sub_losses = {\n",
    "        'pde': 0.0,\n",
    "        'A':   0.0,\n",
    "        'D':   0.0,\n",
    "        'B':   0.0,\n",
    "        'C':   0.0,\n",
    "        'corner': 0.0\n",
    "    }\n",
    "\n",
    "    xf = x_all.flatten()\n",
    "    yf = y_all.flatten()\n",
    "\n",
    "    maskA = torch.isclose(xf, torch.tensor(-L/2, device=xf.device), atol=tol)\n",
    "    maskD = torch.isclose(xf, torch.tensor(+L/2, device=xf.device), atol=tol)\n",
    "    maskB = torch.isclose(yf, torch.tensor(+W/2, device=yf.device), atol=tol)\n",
    "    maskC = torch.isclose(yf, torch.tensor(-W/2, device=yf.device), atol=tol)\n",
    "\n",
    "    mask_pde = ~(maskA | maskD | maskB | maskC)\n",
    "\n",
    "    N = x_all.shape[0]\n",
    "    pointwise_loss = torch.zeros(N, dtype=torch.float32, device=x_all.device)\n",
    "\n",
    "    # PDE\n",
    "    if mask_pde.any():\n",
    "        xp = x_all[mask_pde].view(-1,1)\n",
    "        yp = y_all[mask_pde].view(-1,1)\n",
    "        pde_vals = pde_loss_vectorized(model, xp, yp)\n",
    "        pointwise_loss[mask_pde] = pde_vals\n",
    "        sub_losses['pde'] = float(torch.mean(pde_vals))\n",
    "\n",
    "    # Boundary A\n",
    "    if maskA.any():\n",
    "        xA = x_all[maskA].view(-1,1)\n",
    "        yA = y_all[maskA].view(-1,1)\n",
    "        valsA = boundary_A_loss(model, xA, yA)\n",
    "        pointwise_loss[maskA] = valsA\n",
    "        sub_losses['A'] = float(torch.mean(valsA))\n",
    "\n",
    "    # Boundary D\n",
    "    if maskD.any():\n",
    "        xD = x_all[maskD].view(-1,1)\n",
    "        yD = y_all[maskD].view(-1,1)\n",
    "        valsD = boundary_D_loss(model, xD, yD, L)\n",
    "        pointwise_loss[maskD] = valsD\n",
    "        sub_losses['D'] = float(torch.mean(valsD))\n",
    "\n",
    "    # Boundary B => traction-free top\n",
    "    if maskB.any():\n",
    "        xB = x_all[maskB].view(-1,1)\n",
    "        yB = y_all[maskB].view(-1,1)\n",
    "        valsB = boundary_B_loss(model, xB, yB) / 1e18  # scale\n",
    "        pointwise_loss[maskB] = valsB\n",
    "        sub_losses['B'] = float(torch.mean(valsB))\n",
    "\n",
    "    # Boundary C => traction-free bottom\n",
    "    if maskC.any():\n",
    "        xC = x_all[maskC].view(-1,1)\n",
    "        yC = y_all[maskC].view(-1,1)\n",
    "        valsC = boundary_C_loss(model, xC, yC) / 1e18  # scale\n",
    "        pointwise_loss[maskC] = valsC\n",
    "        sub_losses['C'] = float(torch.mean(valsC))\n",
    "\n",
    "    # Now corner constraint => single point\n",
    "    x_corner = torch.tensor([[-L/2]], dtype=torch.float32, requires_grad=True)\n",
    "    y_corner = torch.tensor([[-W/2]], dtype=torch.float32, requires_grad=True)\n",
    "    corner_vals = corner_loss(model, x_corner, y_corner, weight=corner_weight)\n",
    "    sub_losses['corner'] = float(torch.mean(corner_vals))\n",
    "\n",
    "    # Sum boundary & PDE => mean\n",
    "    total_bc_pde = torch.mean(pointwise_loss)\n",
    "    # Then add corner (not averaged with the rest, your snippet multiplied by big weight)\n",
    "    total_loss = total_bc_pde + corner_vals.mean()\n",
    "\n",
    "    return total_loss, sub_losses\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Single training loop that prints sub-losses\n",
    "# -----------------------------\n",
    "def train_pinn_vectorized(model, optimizer, n_epochs, n_points, L, W):\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # 1) Sample PDE points in full domain: x in [-L/2, L/2], y in [-W/2, W/2]\n",
    "        x_in = (torch.rand((n_points,1)) * L) - (L/2)\n",
    "        y_in = (torch.rand((n_points,1)) * W) - (W/2)\n",
    "\n",
    "        # 2) Sample boundary lines\n",
    "        #  A => x=-L/2\n",
    "        yA = torch.linspace(-W/2, W/2, 100).reshape(-1,1)\n",
    "        xA = -L/2 * torch.ones_like(yA)\n",
    "\n",
    "        #  D => x=+L/2\n",
    "        yD = torch.linspace(-W/2, W/2, 100).reshape(-1,1)\n",
    "        xD = +L/2 * torch.ones_like(yD)\n",
    "\n",
    "        #  B => y=+W/2\n",
    "        xB = torch.linspace(-L/2, L/2, 100).reshape(-1,1)\n",
    "        yB = +W/2 * torch.ones_like(xB)\n",
    "\n",
    "        #  C => y=-W/2\n",
    "        xC = torch.linspace(-L/2, L/2, 100).reshape(-1,1)\n",
    "        yC = -W/2 * torch.ones_like(xC)\n",
    "\n",
    "        # 3) Combine PDE + boundaries\n",
    "        all_x = torch.cat([x_in, xA, xD, xB, xC], dim=0)\n",
    "        all_y = torch.cat([y_in, yA, yD, yB, yC], dim=0)\n",
    "\n",
    "        # 4) Evaluate combined PDE+BC, plus corner\n",
    "        total_loss, sub_dict = combined_loss_vectorized(model, all_x, all_y, L, W, corner_weight=1e9, tol=1e-6)\n",
    "\n",
    "        # 5) Backprop\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_history.append(total_loss.item())\n",
    "\n",
    "        # 6) Print each sub-loss every 500 epochs (or as you wish)\n",
    "        if epoch % 500 == 0:\n",
    "            print(\n",
    "                f\"Epoch {epoch:5d} | total={total_loss.item():.4e} | \"\n",
    "                f\"pde={sub_dict['pde']:.4e}, A={sub_dict['A']:.4e}, \"\n",
    "                f\"D={sub_dict['D']:.4e}, B={sub_dict['B']:.4e}, \"\n",
    "                f\"C={sub_dict['C']:.4e}, corner={sub_dict['corner']:.4e}\"\n",
    "            )\n",
    "\n",
    "    return loss_history\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Run training + Evaluate\n",
    "# -----------------------------\n",
    "model = PINN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 10000\n",
    "n_points = 1000  # PDE interior points each epoch\n",
    "\n",
    "loss_history = train_pinn_vectorized(model, optimizer, n_epochs, n_points, L, W)\n",
    "\n",
    "# ~~~ Evaluate results ~~~\n",
    "file_path = '/Users/murat/Downloads/data.csv'\n",
    "comparison_data = pd.read_csv(file_path)\n",
    "\n",
    "x_values = torch.tensor(comparison_data['X'].values, dtype=torch.float32).reshape(-1, 1)\n",
    "y_values = torch.tensor(comparison_data['Y'].values, dtype=torch.float32).reshape(-1, 1)\n",
    "u_x_actual = comparison_data['u_x_actual'].values\n",
    "u_y_actual = comparison_data['u_y_actual'].values\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    u_pred = model(x_values, y_values)\n",
    "\n",
    "u_x_pred = u_pred[:,0].numpy()\n",
    "u_y_pred = u_pred[:,1].numpy()\n",
    "\n",
    "comparison_data['u_x_pred'] = u_x_pred\n",
    "comparison_data['u_y_pred'] = u_y_pred\n",
    "\n",
    "comparison_data['error_u_x'] = abs(u_x_actual - u_x_pred)\n",
    "comparison_data['error_u_y'] = abs(u_y_actual - u_y_pred)\n",
    "\n",
    "def safe_percent_error(a, p):\n",
    "    if np.isclose(a, 0.0):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return abs(a - p) / abs(a) * 100.0\n",
    "\n",
    "comparison_data['percent_error_u_x'] = [\n",
    "    safe_percent_error(a, p) for a, p in zip(u_x_actual, u_x_pred)\n",
    "]\n",
    "comparison_data['percent_error_u_y'] = [\n",
    "    safe_percent_error(a, p) for a, p in zip(u_y_actual, u_y_pred)\n",
    "]\n",
    "\n",
    "avg_percent_error_x = comparison_data['percent_error_u_x'].mean()\n",
    "avg_percent_error_y = comparison_data['percent_error_u_y'].mean()\n",
    "\n",
    "print(comparison_data)\n",
    "print(f\"Average Percent Error for u_x: {avg_percent_error_x:.2f}%\")\n",
    "print(f\"Average Percent Error for u_y: {avg_percent_error_y:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a88969-8b5c-4f12-82b2-eabedb639431",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
