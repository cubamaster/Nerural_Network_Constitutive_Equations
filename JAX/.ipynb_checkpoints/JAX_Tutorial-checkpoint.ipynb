{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b8ba27",
   "metadata": {},
   "source": [
    "# Murat's JAX tutorial notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f825bc-763d-4590-933a-9e555e01d050",
   "metadata": {},
   "source": [
    "## 0) Setup & Installation (quick notes)\n",
    "\n",
    "JAX has a different instalation process for GPU (specifically CUDA, hence you need a nvidia card), CPU (works for everything else), and hypothetically the Apple Metal (for apple M processors GPU, but at this stage it is in the beta level, and at least I ran into a lot of issues trying to make it run on my mac, so I sticked with CPU)\n",
    "\n",
    "- **CPU‑only (any platform):**\n",
    "  ```bash\n",
    "  pip install -U jax\n",
    "  ```\n",
    "- **NVIDIA GPU (CUDA):** you need a CUDA‑compatible wheel that matches your CUDA/cuDNN.\n",
    "  ```bash\n",
    "  pip install -U \"jax[cuda12]\"\n",
    "  ```\n",
    "- **TPU (Google special processors that are specifically designed to run ML code)**\n",
    "  ```bash\n",
    "  pip install -U \"jax[tpu]\"\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45666681",
   "metadata": {},
   "source": [
    "### quick note, **jax.numpy as jnp** is practically a 1-1 copy of numpy, meaning that you can use all the same functions as in numpy, just with a jnp prefix instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b46c4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.3\n",
      "Platform: macOS-15.6.1-arm64-arm-64bit\n",
      "JAX: 0.5.0\n",
      "JAX devices: [CpuDevice(id=0)]\n",
      "Backend: cpu\n"
     ]
    }
   ],
   "source": [
    "import math, time, functools, sys, platform\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, value_and_grad, vmap, random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"JAX:\", jax.__version__)\n",
    "print(\"JAX devices:\", jax.devices())\n",
    "print(\"Backend:\", jax.default_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18deb641",
   "metadata": {},
   "source": [
    "## 1) JAX arrays: feel like NumPy, run on accelerators\n",
    "\n",
    "**Key ideas**\n",
    "- Use `jax.numpy` as `jnp` (NumPy‑like API).\n",
    "- Arrays live on device (CPU/GPU/TPU) and are **decoders** (functional style).\n",
    "- JAX does not support a dynamic array change, so instead you ave to use `.at[...].set/add/mul` instead of in‑place writes.\n",
    "- Favor **pure functions**: same inputs ⇒ same outputs, no hidden backgroudn or global running states. This is due to the fact that JAX is basically JIT `Just In Time` and makes stuff like autograd very efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6d1e715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [1. 2. 3.]\n",
      "b dtype: int32 | c: [0.   0.25 0.5  0.75 1.  ]\n",
      "a + 10: [11. 12. 13.]\n",
      "a * b: [0. 2. 6.]\n",
      "x original: [0. 0. 0.]\n",
      "x2 updated: [0. 7. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Create arrays, notice that logic is very similar to numpy and/or matlab\n",
    "a = jnp.array([1.0, 2.0, 3.0])\n",
    "b = jnp.arange(3)            # 0,1,2\n",
    "c = jnp.linspace(0, 1, 5)    # 5 points from 0 to 1\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b dtype:\", b.dtype, \"| c:\", c)\n",
    "\n",
    "# Broadcasting like NumPy and/or matlab\n",
    "print(\"a + 10:\", a + 10)\n",
    "print(\"a * b:\", a * b)\n",
    "\n",
    "# No dynamic change, instead use .at to update\n",
    "x = jnp.zeros((3,))\n",
    "x2 = x.at[1].set(7.0)\n",
    "print(\"x original:\", x)\n",
    "print(\"x2 updated:\", x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547aad0f",
   "metadata": {},
   "source": [
    "## 2) Automatic differentiation: `grad` and `value_and_grad`\n",
    "\n",
    "`grad(f)` returns a function that computes **∂f/∂x**. Your function must be **pure** and operate on JAX arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b89eb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x0) = 86.0\n",
      "df/dx at x0 = 32.0\n"
     ]
    }
   ],
   "source": [
    "def f_scalar(x):\n",
    "    return 3.0 * x**2 + 2.0 * x + 1.0\n",
    "\n",
    "df = grad(f_scalar)\n",
    "\n",
    "x0 = 5.0\n",
    "print(\"f(x0) =\", f_scalar(x0))\n",
    "print(\"df/dx at x0 =\", df(x0))\n",
    "\n",
    "# Works on vectorized functions via vmap (next section), or write f to accept vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a1656",
   "metadata": {},
   "source": [
    "## 3) JIT compilation: `@jit`\n",
    "\n",
    "Bascially before every high-level/high-energy computnig use `jit` to compile to XLA for big speedups (especially for large arrays / loops). The first time the function runs is slow, but the subsequent runs will be much faster. This is very helpful since python itself is a very slow language when it comes down to calculation\n",
    "\n",
    "Example: the first call includes compile time; subsequent calls are fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d63b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without JIT:               0.018598 s\n",
      "With JIT (1st call):       0.182805 s\n",
      "With JIT (2nd call):       0.000354 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "# Function without JIT\n",
    "def heavy_poly_nojit(x):\n",
    "    y = 0.0\n",
    "    for i in range(200):\n",
    "        y = y + (i + 1) * x**2 - (i - 1) * x + 3.0\n",
    "    return y\n",
    "\n",
    "# Function with JIT\n",
    "@jit\n",
    "def heavy_poly_jit(x):\n",
    "    y = 0.0\n",
    "    for i in range(200):\n",
    "        y = y + (i + 1) * x**2 - (i - 1) * x + 3.0\n",
    "    return y\n",
    "\n",
    "# Input data\n",
    "x_demo = jnp.linspace(0, 1, 10_000)\n",
    "\n",
    "# --- Run without JIT ---\n",
    "t0 = time.time()\n",
    "_ = heavy_poly_nojit(x_demo).block_until_ready() # block_until_ready just ensures that we capture\n",
    "                                                # the true run time from CPU/GPUT, \n",
    "                                                # we don't need to use it in our research\n",
    "t1 = time.time()\n",
    "\n",
    "# --- Run with JIT (first call: compile + run) ---\n",
    "t2 = time.time()\n",
    "_ = heavy_poly_jit(x_demo).block_until_ready()\n",
    "t3 = time.time()\n",
    "\n",
    "# --- Run with JIT (second call: cached run) ---\n",
    "t4 = time.time()\n",
    "_ = heavy_poly_jit(x_demo).block_until_ready()\n",
    "t5 = time.time()\n",
    "\n",
    "print(f\"Without JIT:               {t1 - t0:.6f} s\")\n",
    "print(f\"With JIT (1st call):       {t3 - t2:.6f} s\")\n",
    "print(f\"With JIT (2nd call):       {t5 - t4:.6f} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed967df",
   "metadata": {},
   "source": [
    "## 5) Batch without for‑loops: `vmap`\n",
    "\n",
    "`vmap` automatically vectorizes a function across a batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "094d64be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched logits shape: (1000,)\n",
      "Single input: 10.0\n",
      "With loop: [ 1.  2.  5. 10. 17.]\n",
      "With vmap: [ 1.  2.  5. 10. 17.]\n"
     ]
    }
   ],
   "source": [
    "# A simple function: square and add 1\n",
    "def simple_fn(x):\n",
    "    return x**2 + 1\n",
    "\n",
    "# A single input\n",
    "print(\"Single input:\", simple_fn(3.0))   # → 10.0\n",
    "\n",
    "# A batch of inputs\n",
    "xs = jnp.arange(5.0)   # [0, 1, 2, 3, 4]\n",
    "\n",
    "# --- Without vmap (manual loop) ---\n",
    "results_loop = jnp.array([simple_fn(x) for x in xs])\n",
    "print(\"With loop:\", results_loop)\n",
    "\n",
    "# --- With vmap (automatic batching) ---\n",
    "results_vmap = vmap(simple_fn)(xs)\n",
    "print(\"With vmap:\", results_vmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2987e9-83d9-4914-ba4d-d0b290574d9b",
   "metadata": {},
   "source": [
    "### **now into more complicated example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b152852-af6e-45d2-9575-a5c265da9776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched logits shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random, vmap\n",
    "\n",
    "# Define a function for one input vector\n",
    "def single_logit(params, x):\n",
    "    \"\"\"\n",
    "    Compute a single logit: w·x + b\n",
    "    - params is a tuple (w, b):\n",
    "        w = weight vector\n",
    "        b = bias (a scalar)\n",
    "    - x is one input vector\n",
    "    \"\"\"\n",
    "    w, b = params\n",
    "    return jnp.dot(w, x) + b  # dot product = sum of w[i] * x[i]\n",
    "\n",
    "# -------------------------------\n",
    "# Create random parameters and data\n",
    "# -------------------------------\n",
    "key = random.PRNGKey(42)         # PRNG key (needed in JAX for reproducible randomness)\n",
    "key, kW, kX = random.split(key, 3)  # split into subkeys\n",
    "\n",
    "# Generate random weights for 3 features\n",
    "w = random.normal(kW, (3,))      # shape (3,)\n",
    "b = 0.5                          # bias is just a number\n",
    "params = (w, b)                  # bundle them into a tuple\n",
    "\n",
    "# Generate a dataset of 1000 input vectors, each of size 3\n",
    "X = random.normal(kX, (1000, 3)) # shape (1000, 3)\n",
    "\n",
    "# -------------------------------\n",
    "# Apply single_logit to every row of X\n",
    "# -------------------------------\n",
    "\n",
    "# Normally, we would loop over rows of X like this:\n",
    "# logits = jnp.array([single_logit(params, x) for x in X])\n",
    "\n",
    "# Instead, vmap does this automatically and efficiently:\n",
    "batched_logits = vmap(lambda x: single_logit(params, x))(X)\n",
    "\n",
    "print(\"Batched logits shape:\", batched_logits.shape)\n",
    "# -> (1000,)   because each of the 1000 inputs produces one logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f83f7",
   "metadata": {},
   "source": [
    "## 6) Pytrees (nested parameter containers)\n",
    "\n",
    "A **pytree** is any nested structure of lists/tuples/dicts of arrays. JAX knows how to map/stack/flatten them.\n",
    "\n",
    "**tuple** exist only in python, and it is bascially an array of different types of variables, and it can very by dimensions as well \n",
    "Common ops:\n",
    "- `jax.tree_util.tree_map(fn, pytree)`\n",
    "- `jax.tree_util.tree_flatten(pytree)`\n",
    "- People usually store NN weights as a dict of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a442f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num leaves: 4\n",
      "First leaf shape: (2, 3)\n",
      "Restored pytree OK.\n"
     ]
    }
   ],
   "source": [
    "from jax import tree_util\n",
    "\n",
    "params = {\n",
    "    \"layer1\": {\"W\": jnp.ones((2, 3)), \"b\": jnp.zeros((3,))},\n",
    "    \"layer2\": {\"W\": jnp.ones((3, 1))*2, \"b\": jnp.array([0.1])},\n",
    "}\n",
    "\n",
    "# Apply a function to every array in the pytree\n",
    "scaled = tree_util.tree_map(lambda x: x * 0.5, params)\n",
    "\n",
    "leaves, treedef = tree_util.tree_flatten(params)\n",
    "print(\"Num leaves:\", len(leaves))\n",
    "print(\"First leaf shape:\", leaves[0].shape)\n",
    "\n",
    "# Restore from leaves + structure\n",
    "restored = tree_util.tree_unflatten(treedef, leaves)\n",
    "assert all([jnp.allclose(a, b) for a,b in zip(leaves, tree_util.tree_flatten(restored)[0])])\n",
    "print(\"Restored pytree OK.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
