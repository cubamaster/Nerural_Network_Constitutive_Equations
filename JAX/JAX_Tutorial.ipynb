{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b8ba27",
   "metadata": {},
   "source": [
    "# Murat's JAX tutorial notebook\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f825bc-763d-4590-933a-9e555e01d050",
   "metadata": {},
   "source": [
    "## 0) Setup & Installation (quick notes)\n",
    "\n",
    "JAX has a different instalation process for GPU (specifically CUDA, hence you need a nvidia card), CPU (works for everything else), and hypothetically the Apple Metal (for apple M processors GPU, but at this stage it is in the beta level, and at least I ran into a lot of issues trying to make it run on my mac, so I sticked with CPU)\n",
    "\n",
    "- **CPU‑only (any platform):**\n",
    "  ```bash\n",
    "  pip install -U jax\n",
    "  ```\n",
    "- **NVIDIA GPU (CUDA):** you need a CUDA‑compatible wheel that matches your CUDA/cuDNN.\n",
    "  ```bash\n",
    "  pip install -U \"jax[cuda12]\"\n",
    "  ```\n",
    "- **TPU (Google special processors that are specifically designed to run ML code)**\n",
    "  ```bash\n",
    "  pip install -U \"jax[tpu]\"\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45666681",
   "metadata": {},
   "source": [
    "### quick note, **jax.numpy as jnp** is practically a 1-1 copy of numpy, meaning that you can use all the same functions as in numpy, just with a jnp prefix instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b46c4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.12.3\n",
      "Platform: macOS-15.6.1-arm64-arm-64bit\n",
      "JAX: 0.5.0\n",
      "JAX devices: [CpuDevice(id=0)]\n",
      "Backend: cpu\n"
     ]
    }
   ],
   "source": [
    "import math, time, functools, sys, platform\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, grad, value_and_grad, vmap, random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"JAX:\", jax.__version__)\n",
    "print(\"JAX devices:\", jax.devices())\n",
    "print(\"Backend:\", jax.default_backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18deb641",
   "metadata": {},
   "source": [
    "## 1) JAX arrays: feel like NumPy, run on accelerators\n",
    "\n",
    "**Key ideas**\n",
    "- Use `jax.numpy` as `jnp` (NumPy‑like API).\n",
    "- Arrays live on device (CPU/GPU/TPU) and are **decoders** (functional style).\n",
    "- JAX does not support a dynamic array change, so instead you ave to use `.at[...].set/add/mul` instead of in‑place writes.\n",
    "- Favor **pure functions**: same inputs ⇒ same outputs, no hidden backgroudn or global running states. This is due to the fact that JAX is basically JIT `Just In Time` and makes stuff like autograd very efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6d1e715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: [1. 2. 3.]\n",
      "b dtype: int32 | c: [0.   0.25 0.5  0.75 1.  ]\n",
      "a + 10: [11. 12. 13.]\n",
      "a * b: [0. 2. 6.]\n",
      "x original: [0. 0. 0.]\n",
      "x2 updated: [0. 7. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Create arrays, notice that logic is very similar to numpy and/or matlab\n",
    "a = jnp.array([1.0, 2.0, 3.0])\n",
    "b = jnp.arange(3)            # 0,1,2\n",
    "c = jnp.linspace(0, 1, 5)    # 5 points from 0 to 1\n",
    "\n",
    "print(\"a:\", a)\n",
    "print(\"b dtype:\", b.dtype, \"| c:\", c)\n",
    "\n",
    "# Broadcasting like NumPy and/or matlab\n",
    "print(\"a + 10:\", a + 10)\n",
    "print(\"a * b:\", a * b)\n",
    "\n",
    "# No dynamic change, instead use .at to update\n",
    "x = jnp.zeros((3,))\n",
    "x2 = x.at[1].set(7.0)\n",
    "print(\"x original:\", x)\n",
    "print(\"x2 updated:\", x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547aad0f",
   "metadata": {},
   "source": [
    "## 2) Automatic differentiation: `grad` and `value_and_grad`\n",
    "\n",
    "`grad(f)` returns a function that computes **∂f/∂x**. Your function must be **pure** and operate on JAX arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b89eb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x0) = 86.0\n",
      "df/dx at x0 = 32.0\n"
     ]
    }
   ],
   "source": [
    "def f_scalar(x):\n",
    "    return 3.0 * x**2 + 2.0 * x + 1.0\n",
    "\n",
    "df = grad(f_scalar)\n",
    "\n",
    "x0 = 5.0\n",
    "print(\"f(x0) =\", f_scalar(x0))\n",
    "print(\"df/dx at x0 =\", df(x0))\n",
    "\n",
    "# Works on vectorized functions via vmap (next section), or write f to accept vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a1656",
   "metadata": {},
   "source": [
    "**JAX accelerators like JIT works equaly on Mac Silicon and x84/GPU processor units**\n",
    "\n",
    " The JAX accelerators will work natively on the deivce that you told JAX to work on. Hence if you installed the CPU version only, it will run on CPU and not GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5682415-eaa6-4787-8706-f1459aeee13f",
   "metadata": {},
   "source": [
    "## 3) JIT compilation: `@jit`\n",
    "\n",
    "\n",
    "Bascially before every high-level/high-energy computnig use `jit` to compile to XLA for big speedups (especially for large arrays / loops). The first time the function runs is slow, but the subsequent runs will be much faster. This is very helpful since python itself is a very slow language when it comes down to calculation\n",
    "\n",
    "Example: the first call includes compile time; subsequent calls are fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d63b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without JIT:               0.018598 s\n",
      "With JIT (1st call):       0.182805 s\n",
      "With JIT (2nd call):       0.000354 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "# Function without JIT\n",
    "def heavy_poly_nojit(x):\n",
    "    y = 0.0\n",
    "    for i in range(200):\n",
    "        y = y + (i + 1) * x**2 - (i - 1) * x + 3.0\n",
    "    return y\n",
    "\n",
    "# Function with JIT\n",
    "@jit\n",
    "def heavy_poly_jit(x):\n",
    "    y = 0.0\n",
    "    for i in range(200):\n",
    "        y = y + (i + 1) * x**2 - (i - 1) * x + 3.0\n",
    "    return y\n",
    "\n",
    "# Input data\n",
    "x_demo = jnp.linspace(0, 1, 10_000)\n",
    "\n",
    "# --- Run without JIT ---\n",
    "t0 = time.time()\n",
    "_ = heavy_poly_nojit(x_demo).block_until_ready() # block_until_ready just ensures that we capture\n",
    "                                                # the true run time from CPU/GPUT, \n",
    "                                                # we don't need to use it in our research\n",
    "t1 = time.time()\n",
    "\n",
    "# --- Run with JIT (first call: compile + run) ---\n",
    "t2 = time.time()\n",
    "_ = heavy_poly_jit(x_demo).block_until_ready()\n",
    "t3 = time.time()\n",
    "\n",
    "# --- Run with JIT (second call: cached run) ---\n",
    "t4 = time.time()\n",
    "_ = heavy_poly_jit(x_demo).block_until_ready()\n",
    "t5 = time.time()\n",
    "\n",
    "print(f\"Without JIT:               {t1 - t0:.6f} s\")\n",
    "print(f\"With JIT (1st call):       {t3 - t2:.6f} s\")\n",
    "print(f\"With JIT (2nd call):       {t5 - t4:.6f} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed967df",
   "metadata": {},
   "source": [
    "## 5) Batch without for‑loops: `vmap`\n",
    "\n",
    "`vmap` automatically vectorizes a function across a batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "094d64be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched logits shape: (1000,)\n",
      "Single input: 10.0\n",
      "With loop: [ 1.  2.  5. 10. 17.]\n",
      "With vmap: [ 1.  2.  5. 10. 17.]\n"
     ]
    }
   ],
   "source": [
    "# A simple function: square and add 1\n",
    "def simple_fn(x):\n",
    "    return x**2 + 1\n",
    "\n",
    "# A single input\n",
    "print(\"Single input:\", simple_fn(3.0))   # → 10.0\n",
    "\n",
    "# A batch of inputs\n",
    "xs = jnp.arange(5.0)   # [0, 1, 2, 3, 4]\n",
    "\n",
    "# --- Without vmap (manual loop) ---\n",
    "results_loop = jnp.array([simple_fn(x) for x in xs])\n",
    "print(\"With loop:\", results_loop)\n",
    "\n",
    "# --- With vmap (automatic batching) ---\n",
    "results_vmap = vmap(simple_fn)(xs)\n",
    "print(\"With vmap:\", results_vmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2987e9-83d9-4914-ba4d-d0b290574d9b",
   "metadata": {},
   "source": [
    "### **now into more complicated example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b152852-af6e-45d2-9575-a5c265da9776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched logits shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import random, vmap\n",
    "\n",
    "# Define a function for one input vector\n",
    "def single_logit(params, x):\n",
    "    \"\"\"\n",
    "    Compute a single logit: w·x + b\n",
    "    - params is a tuple (w, b):\n",
    "        w = weight vector\n",
    "        b = bias (a scalar)\n",
    "    - x is one input vector\n",
    "    \"\"\"\n",
    "    w, b = params\n",
    "    return jnp.dot(w, x) + b  # dot product = sum of w[i] * x[i]\n",
    "\n",
    "# -------------------------------\n",
    "# Create random parameters and data\n",
    "# -------------------------------\n",
    "key = random.PRNGKey(42)         # PRNG key (needed in JAX for reproducible randomness)\n",
    "key, kW, kX = random.split(key, 3)  # split into subkeys\n",
    "\n",
    "# Generate random weights for 3 features\n",
    "w = random.normal(kW, (3,))      # shape (3,)\n",
    "b = 0.5                          # bias is just a number\n",
    "params = (w, b)                  # bundle them into a tuple\n",
    "\n",
    "# Generate a dataset of 1000 input vectors, each of size 3\n",
    "X = random.normal(kX, (1000, 3)) # shape (1000, 3)\n",
    "\n",
    "# -------------------------------\n",
    "# Apply single_logit to every row of X\n",
    "# -------------------------------\n",
    "\n",
    "# Normally, we would loop over rows of X like this:\n",
    "# logits = jnp.array([single_logit(params, x) for x in X])\n",
    "\n",
    "# Instead, vmap does this automatically and efficiently:\n",
    "batched_logits = vmap(lambda x: single_logit(params, x))(X)\n",
    "\n",
    "print(\"Batched logits shape:\", batched_logits.shape)\n",
    "# -> (1000,)   because each of the 1000 inputs produces one logit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f83f7",
   "metadata": {},
   "source": [
    "## 6) Pytrees (nested parameter containers)\n",
    "\n",
    "A **pytree** is any nested structure of lists/tuples/dicts of arrays. JAX knows how to map/stack/flatten them.\n",
    "\n",
    "**tuple** exist only in python, and it is bascially an array of different types of variables, and it can very by dimensions as well \n",
    "Common ops:\n",
    "- `jax.tree_util.tree_map(fn, pytree)`\n",
    "- `jax.tree_util.tree_flatten(pytree)`\n",
    "- People usually store NN weights as a dict of arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a442f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num leaves: 4\n",
      "First leaf shape: (2, 3)\n",
      "Restored pytree OK.\n"
     ]
    }
   ],
   "source": [
    "from jax import tree_util\n",
    "\n",
    "params = {\n",
    "    \"layer1\": {\"W\": jnp.ones((2, 3)), \"b\": jnp.zeros((3,))},\n",
    "    \"layer2\": {\"W\": jnp.ones((3, 1))*2, \"b\": jnp.array([0.1])},\n",
    "}\n",
    "\n",
    "# Apply a function to every array in the pytree\n",
    "scaled = tree_util.tree_map(lambda x: x * 0.5, params)\n",
    "\n",
    "leaves, treedef = tree_util.tree_flatten(params)\n",
    "print(\"Num leaves:\", len(leaves))\n",
    "print(\"First leaf shape:\", leaves[0].shape)\n",
    "\n",
    "# Restore from leaves + structure\n",
    "restored = tree_util.tree_unflatten(treedef, leaves)\n",
    "assert all([jnp.allclose(a, b) for a,b in zip(leaves, tree_util.tree_flatten(restored)[0])])\n",
    "print(\"Restored pytree OK.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
