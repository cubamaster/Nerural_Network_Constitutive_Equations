{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67463517-7e9a-4e2f-83e0-a06858938373",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0 │ Total: 3.187e+02 │ PDE+comp: 3.135e-01 │ BC: 6.368e+00\n",
      "Epoch   500 │ Total: 4.960e-04 │ PDE+comp: 3.901e-04 │ BC: 2.118e-06\n",
      "Epoch  1000 │ Total: 2.023e-04 │ PDE+comp: 1.532e-04 │ BC: 9.832e-07\n",
      "Epoch  1500 │ Total: 1.040e-04 │ PDE+comp: 8.015e-05 │ BC: 4.772e-07\n",
      "Epoch  2000 │ Total: 6.557e-05 │ PDE+comp: 5.272e-05 │ BC: 2.570e-07\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 188\u001b[0m\n\u001b[1;32m    186\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m    187\u001b[0m n_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m--> 188\u001b[0m loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_pinn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpinn_elasticity.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Model weights saved to pinn_elasticity.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 174\u001b[0m, in \u001b[0;36mtrain_pinn\u001b[0;34m(model, optimizer, n_epochs, n_points, L, W)\u001b[0m\n\u001b[1;32m    171\u001b[0m loss \u001b[38;5;241m=\u001b[39m w_pde\u001b[38;5;241m*\u001b[39mlp \u001b[38;5;241m+\u001b[39m w_bc\u001b[38;5;241m*\u001b[39mlb\n\u001b[1;32m    173\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 174\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    177\u001b[0m history\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.tri as mtri\n",
    "\n",
    "# Physical constants\n",
    "L, W = 1.0, 0.5    # domain size (cm)\n",
    "lambda_, mu = 5e9, 5e9  # elastic constants (Pa)\n",
    "h = 1.0            # thickness (cm)\n",
    "sf = 1e9           # stress scaling\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINN, self).__init__()\n",
    "        # now outputs: u_x, u_y, E_xx, E_yy, E_xy\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.Softplus(beta=10),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.Softplus(beta=10),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Softplus(beta=10),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Softplus(beta=10),\n",
    "            nn.Linear(256, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # normalize into [-1,1]\n",
    "        xi  = 2.0 * x / L\n",
    "        eta = 2.0 * y / W\n",
    "\n",
    "        raw = self.net(torch.cat([xi, eta], dim=1))\n",
    "        return raw\n",
    "\n",
    "# compute true displacement gradients and small‐strain tensor via autograd\n",
    "def strain_tensor(u_net, v_net, x, y):\n",
    "    # ∂u/∂x, ∂u/∂y\n",
    "    u_x_true = torch.autograd.grad(u_net, x,\n",
    "                    grad_outputs=torch.ones_like(u_net),\n",
    "                    retain_graph=True, create_graph=True)[0]\n",
    "    u_y_true = torch.autograd.grad(u_net, y,\n",
    "                    grad_outputs=torch.ones_like(u_net),\n",
    "                    retain_graph=True, create_graph=True)[0]\n",
    "    # ∂v/∂x, ∂v/∂y\n",
    "    v_x_true = torch.autograd.grad(v_net, x,\n",
    "                    grad_outputs=torch.ones_like(v_net),\n",
    "                    retain_graph=True, create_graph=True)[0]\n",
    "    v_y_true = torch.autograd.grad(v_net, y,\n",
    "                    grad_outputs=torch.ones_like(v_net),\n",
    "                    retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    # small‐strain components\n",
    "    Exx = u_x_true\n",
    "    Eyy = v_y_true\n",
    "    Exy = 0.5 * (u_y_true + v_x_true)\n",
    "\n",
    "    return u_x_true, u_y_true, v_x_true, v_y_true, Exx, Eyy, Exy\n",
    "\n",
    "\n",
    "# stress from strain (unchanged)\n",
    "def stress_tensor(Exx, Eyy, Exy):\n",
    "    TrE = Exx + Eyy\n",
    "    Sxx = h * ((lambda_/sf) * TrE + 2 * (mu/sf) * Exx)\n",
    "    Syy = h * ((lambda_/sf) * TrE + 2 * (mu/sf) * Eyy)\n",
    "    Sxy = h * (2 * (mu/sf) * Exy)\n",
    "    return Sxx, Syy, Sxy\n",
    "\n",
    "\n",
    "# FO‐PINN physics loss with six‐output network: [u, v, u_x, u_y, v_x, v_y]\n",
    "def physics_loss(model, x, y):\n",
    "    x.requires_grad_(True)\n",
    "    y.requires_grad_(True)\n",
    "\n",
    "    out = model(x, y)\n",
    "    # unpack network outputs\n",
    "    u_net, v_net, u_x_net, u_y_net, v_x_net, v_y_net = (\n",
    "        out[:,  i:i+1] for i in range(6)\n",
    "    )\n",
    "\n",
    "    # 1) true gradients & strains via autograd\n",
    "    u_x_true, u_y_true, v_x_true, v_y_true, \\\n",
    "      Exx_true, Eyy_true, Exy_true = strain_tensor(u_net, v_net, x, y)\n",
    "\n",
    "    # 2) compatibility loss\n",
    "    w_comp = 1.0\n",
    "    loss_grad = (\n",
    "        torch.mean((u_x_net - u_x_true)**2) +\n",
    "        torch.mean((u_y_net - u_y_true)**2) +\n",
    "        torch.mean((v_x_net - v_x_true)**2) +\n",
    "        torch.mean((v_y_net - v_y_true)**2)\n",
    "    )\n",
    "\n",
    "    lc = (loss_grad) * w_comp\n",
    "    \n",
    "    Exx_net, Eyy_net = u_x_net, v_y_net\n",
    "    Exy_net = 0.5 * (u_y_net + v_x_net)\n",
    "    Sxx, Syy, Sxy = stress_tensor(Exx_net, Eyy_net, Exy_net)\n",
    "\n",
    "    # 4) first‐order PDE residuals\n",
    "    Sxx_x = torch.autograd.grad(Sxx, x,\n",
    "                 grad_outputs=torch.ones_like(Sxx),\n",
    "                 retain_graph=True, create_graph=True)[0]\n",
    "    Sxy_y = torch.autograd.grad(Sxy, y,\n",
    "                 grad_outputs=torch.ones_like(Sxy),\n",
    "                 retain_graph=True, create_graph=True)[0]\n",
    "    Syy_y = torch.autograd.grad(Syy, y,\n",
    "                 grad_outputs=torch.ones_like(Syy),\n",
    "                 retain_graph=True, create_graph=True)[0]\n",
    "    Sxy_x = torch.autograd.grad(Sxy, x,\n",
    "                 grad_outputs=torch.ones_like(Sxy),\n",
    "                 retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "    rx = Sxx_x + Sxy_y\n",
    "    ry = Syy_y + Sxy_x\n",
    "    lpde = torch.mean(rx**2 + ry**2)\n",
    "\n",
    "    return lpde + lc\n",
    "\n",
    "\n",
    "# BC loss\n",
    "def boundary_condition_loss(model, L, W):\n",
    "    w_A, w_D, w_C, w_B = 1.0, 1.0, 1.0, 1.0 # Weight for each \n",
    "                                            # side of the boundary condition\n",
    "    \n",
    "    # A: x = -L/2, u=v=0\n",
    "    y_A = torch.linspace(-W/2, W/2, 500).reshape(-1,1)\n",
    "    x_A = -L/2 * torch.ones_like(y_A)\n",
    "    out_A = model(x_A, y_A)\n",
    "    u_A, v_A = out_A[:,0:1], out_A[:,1:2]\n",
    "    loss_A = torch.mean(u_A**2 + v_A**2)\n",
    "    \n",
    "    # D: x = +L/2, u_x = 0.025L, u_y = 0\n",
    "    x_D = L/2 * torch.ones_like(y_A)\n",
    "    out_D = model(x_D, y_A)\n",
    "    u_D, v_D = out_D[:,2:3], out_D[:,3:4]\n",
    "    loss_D = torch.mean((u_D - 0.02*L)**2 + v_D**2)\n",
    "    \n",
    "    # C: y = -W/2 traction-free -> sigma_yy=0, sigma_xy=0\n",
    "    x_C = torch.linspace(-L/2, L/2, 200).reshape(-1,1)\n",
    "    y_C = -W/2 * torch.ones_like(x_C)\n",
    "    out_C = model(x_C, y_C)\n",
    "    Exx_C, Eyy_C = out_C[:,2:3], out_C[:,5:6]\n",
    "    Exy_C = 0.5*(out_C[:,3:4] + out_C[:,4:5])\n",
    "    _, Syy_C, Sxy_C = stress_tensor(Exx_C, Eyy_C, Exy_C)\n",
    "    loss_C = torch.mean(Syy_C**2 + Sxy_C**2)\n",
    "    \n",
    "    # B: y = +W/2 traction-free -> sigma_yy=0, sigma_xy=0\n",
    "    y_B = W/2 * torch.ones_like(x_C)\n",
    "    out_B = model(x_C, y_B)\n",
    "    Exx_B, Eyy_B = out_B[:,2:3], out_B[:,5:6]\n",
    "    Exy_B = 0.5*(out_B[:,3:4] + out_B[:,4:5])\n",
    "    _, Syy_B, Sxy_B = stress_tensor(Exx_B, Eyy_B, Exy_B)\n",
    "    loss_B = torch.mean(Syy_B**2 + Sxy_B**2)\n",
    "    return w_A*loss_A + w_D*loss_D + w_C*loss_C + w_B*loss_B\n",
    "\n",
    "\n",
    "# train \n",
    "def train_pinn(model, optimizer, n_epochs, n_points, L, W):\n",
    "    history = []\n",
    "    w_pde, w_bc = 1.0, 50.0\n",
    "\n",
    "    for ep in range(n_epochs):\n",
    "        x = torch.rand(n_points,1)*L - L/2\n",
    "        y = torch.rand(n_points,1)*W - W/2\n",
    "\n",
    "        lp = physics_loss(model, x, y)           \n",
    "        lb = boundary_condition_loss(model, L, W)  \n",
    "        loss = w_pde*lp + w_bc*lb\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        history.append(loss.item())\n",
    "        if ep % 500 == 0:\n",
    "            print(f\"Epoch {ep:5d} │ Total: {loss.item():.3e} │ PDE+comp: {lp.item():.3e} │ BC: {lb.item():.3e}\")\n",
    "    return history\n",
    "\n",
    "# initialize and run\n",
    "model     = PINN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 10000\n",
    "n_points = 1000\n",
    "loss_history = train_pinn(model, optimizer, n_epochs, n_points, L, W)\n",
    "\n",
    "torch.save(model.state_dict(), \"pinn_elasticity.pth\")\n",
    "print(\"✅ Model weights saved to pinn_elasticity.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdbfb10-fac4-4633-8adf-0df99dd1120e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
